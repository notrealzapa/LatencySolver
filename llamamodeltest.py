# -*- coding: utf-8 -*-
"""LlamaModelTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10gtWfX-lRd0F86XbAApz0V18EmRxarYT
"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
import time

class QuantumInspiredLLaMA3(nn.Module):
    def __init__(self):
        super(QuantumInspiredLLaMA3, self).__init__()
        self.model = AutoModelForCausalLM.from_pretrained("meta-llama/LLaMA-3")

        self.custom_layers = nn.Sequential(
            self.superposition([nn.Linear(4096, 1024), nn.ReLU(), nn.Linear(1024, 2048)]),
            self.superposition([nn.Linear(2048, 1024), nn.ReLU(), nn.Linear(1024, 512)])
        )

        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def superposition(self, layers):
        """
        Applies the superposition on a group of layers.
        Mean average as the superposition method.
        """
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.model(x)[0]
        x = torch.flatten(x, 1)
        x = self.custom_layers(x)
        x = self.classifier(x)
        return x

quantum_llama3 = QuantumInspiredLLaMA3()

tokenizer = AutoTokenizer.from_pretrained("meta-llama/LLaMA-3")

def tokenize_texts(texts):
    return tokenizer(texts, padding='max_length', truncation=True, return_tensors="pt", max_length=512)

train_texts = ["your text data here..."]
train_labels = torch.tensor([1] * len(train_texts))
train_inputs = tokenize_texts(train_texts)
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(quantum_llama3.parameters(), lr=0.001)

def train_model(model, train_loader, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs.squeeze(), labels.float())
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            predicted = (outputs.squeeze() > 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy: {100 * correct / total:.2f}%')

def measure_inference_time(model, test_loader):
    model.eval()
    start_time = time.time()
    with torch.no_grad():
        for inputs, _ in test_loader:
            outputs = model(inputs)
    end_time = time.time()
    print(f"Inference Time: {end_time - start_time:.4f} seconds")

print("Training Quantum-Inspired LLaMA-3...")
train_model(quantum_llama3, train_loader, criterion, optimizer, num_epochs=5)

#PS need to create a separate test dataset and loader for evaluation and inference time measurement.